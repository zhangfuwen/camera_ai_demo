from flask import Flask, request, jsonify, Response, send_from_directory
from flask_cors import CORS  # Add this import
import cv2
import numpy as np
from PIL import Image
from ultralytics import YOLO
import io
import base64
import json
import os
from datetime import datetime
import werkzeug.utils
import threading
import time
import random
import asyncio
from flask_socketio import SocketIO, emit
from openai import OpenAI

app = Flask(__name__, static_folder='.', template_folder='.')
CORS(app)  # Enable CORS for all routes
socketio = SocketIO(app, cors_allowed_origins="*")

# Global data storage for streaming
stream_data = {
    'sensor_values': {},
    'user_emotions': {},
    'audio_detections': [],
    'video_detections': [],
    'overall_status': {}
}

# Analysis results storage for summary generation
analysis_results = {
    'video_analysis': [],  # Store video analysis results
    'audio_analysis': [],  # Store audio analysis results
    'emotion_history': [], # Store emotion tracking data
    'activity_history': [] # Store activity tracking data
}

# Configuration for storage limits
MAX_STORED_RESULTS = 50  # Keep last 50 results of each type

# Connected clients
connected_clients = set()

# Gemini client configuration (using OpenAI library)
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', 'sk-EplRjGkWQ9CwXK5w10936448E56a46BcB487EeE809C6Bd40')
gemini_client = None

try:
    gemini_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url="https://api.apiyi.com/v1"
    )
    print('[GEMINI] Client initialized successfully with OpenAI library')
except Exception as e:
    print(f'[GEMINI] Failed to initialize client: {e}')
    gemini_client = None

# Load the trained food detection model
# First try to load the trained model from the training directory
#model_path = "food_detection_training/runs/detect/food_det_model/weights/best.pt"
# model_path = "yolov8n-seg.pt"
model_path = "best.pt"
model_path = "./food_detection_training/runs/detect/food_det_model/weights/best.pt"
model_path = "./food_detection_training/yolov8n-seg.pt"
model_path = "yolo26x.pt"
fallback_model_path = "yolov8n.pt"

print("Loading food detection model...")
try:
    model = YOLO(model_path)
    print(f"Loaded trained model from {model_path}")
except Exception as e:
    print(f"Could not load trained model: {e}")
    print(f"Loading default YOLOv8 model instead...")
    model = YOLO(fallback_model_path)

# Get the model's class names
try:
    class_names = model.names  # This gets the class names from the trained model
    print(f"Model loaded with {len(class_names)} classes: {list(class_names.values())}")
except:
    class_names = {}  # Fallback if class names aren't available
    print("Could not retrieve class names from model")

# Create upload directory for videos if it doesn't exist
UPLOAD_FOLDER = 'recorded_videos'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
    print(f"Created upload directory: {UPLOAD_FOLDER}")

# Create upload directory for audio if it doesn't exist
AUDIO_UPLOAD_FOLDER = 'recorded_audio'
if not os.path.exists(AUDIO_UPLOAD_FOLDER):
    os.makedirs(AUDIO_UPLOAD_FOLDER)
    print(f"Created audio upload directory: {AUDIO_UPLOAD_FOLDER}")

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['AUDIO_UPLOAD_FOLDER'] = AUDIO_UPLOAD_FOLDER



@app.route('/')
def home():
    """Serve the main HTML page"""
    return send_from_directory('.', 'index.html')

@app.route('/<path:path>')
def serve_static(path):
    """Serve static files like CSS, JS, images, etc."""
    return send_from_directory('.', path)

@app.route('/detect_food', methods=['POST'])
def detect_food():
    """
    Handle POST requests with base64 encoded images for YOLO segmentation-based food detection.
    
    Expected JSON format:
    {
        "image": "base64_encoded_image_string"
    }
    
    Returns:
    {
        "success": true,
        "detections": [
            {
                "label": "food_item_name",
                "score": confidence_score,
                "box": {"xmin": x1, "ymin": y1, "xmax": x2, "ymax": y2},
                "mask": [[0, 0, 1, 1, ...], ...]  // 2D array representing the segmentation mask
            }
        ]
    }
    """
    try:
        data = request.get_json()
        if not data or 'image' not in data:
            return jsonify({"success": False, "error": "No image data provided"}), 400
        
        # Decode base64 image
        image_data = base64.b64decode(data['image'])
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
      #  img = img.transpose(1, 0, 2)
        
        if img is None:
            return jsonify({"success": False, "error": "Could not decode image"}), 400

        print("Image shape:", img.shape)
        
        # Perform segmentation with YOLO
        results = model(img, conf=0.1)  # Confidence threshold of 0.5
        
        detections = []
        
        # Process detections
        if results and len(results) > 0:
            r = results[0]
            
            # Get boxes, scores, and class IDs
            boxes = r.boxes.xyxy.cpu().numpy()  # Bounding boxes
            scores = r.boxes.conf.cpu().numpy()  # Confidence scores
            class_ids = r.boxes.cls.cpu().numpy()  # Class IDs
            
            # Get masks if available
            masks = r.masks.data.cpu().numpy() if r.masks is not None else None
            
            for i in range(len(boxes)):
                box = boxes[i]
                score = float(scores[i])
                class_id = int(class_ids[i])
                
                # Convert class ID to label - all models have their class names embedded
                label = model.names.get(class_id, f"Class {class_id}")
                
                # Include all detections regardless of class
                # Prepare detection data
                detection = {
                    "label": label,
                    "score": score,
                    "box": {
                        "xmin": float(box[0]),
                        "ymin": float(box[1]),
                        "xmax": float(box[2]),
                        "ymax": float(box[3])
                    }
                }
                
                # Add mask data if available
                if masks is not None and i < len(masks):
                    # Convert mask tensor to a 2D array of 0s and 1s
                    mask_tensor = masks[i]
                    # Convert to binary mask and then to list format
                    mask_array = (mask_tensor > 0.5).astype(int).tolist()
                    detection["mask"] = mask_array
                
                detections.append(detection)
        
        return jsonify({
            "success": True,
            "detections": detections
        })
    
    except Exception as e:
        app.logger.error(f"Error in detect_food: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/detect_food_stream', methods=['POST'])
def detect_food_stream():
    """
    Handle POST requests with base64 encoded images for YOLO segmentation-based food detection.
    Same functionality as /detect_food but with a different endpoint name.
    
    Expected JSON format:
    {
        "image": "base64_encoded_image_string"
    }
    
    Returns:
    {
        "success": true,
        "detections": [
            {
                "label": "food_item_name",
                "score": confidence_score,
                "box": {"xmin": x1, "ymin": y1, "xmax": x2, "ymax": y2},
                "mask": [[0, 0, 1, 1, ...], ...]  // 2D array representing the segmentation mask
            }
        ]
    }
    """
    try:
        data = request.get_json()
        if not data or 'image' not in data:
            return jsonify({"success": False, "error": "No image data provided"}), 400
        
        # Decode base64 image
        image_data = base64.b64decode(data['image'])
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is None:
            return jsonify({"success": False, "error": "Could not decode image"}), 400
        
        # Perform segmentation with YOLO
        results = model(img, conf=0.5)  # Confidence threshold of 0.5
        
        detections = []
        
        # Process detections
        if results and len(results) > 0:
            r = results[0]
            
            # Get boxes, scores, and class IDs
            boxes = r.boxes.xyxy.cpu().numpy()  # Bounding boxes
            scores = r.boxes.conf.cpu().numpy()  # Confidence scores
            class_ids = r.boxes.cls.cpu().numpy()  # Class IDs
            
            # Get masks if available
            masks = r.masks.data.cpu().numpy() if r.masks is not None else None
            
            for i in range(len(boxes)):
                box = boxes[i]
                score = float(scores[i])
                class_id = int(class_ids[i])
                
                # Convert class ID to label - all models have their class names embedded
                label = model.names.get(class_id, f"Class {class_id}")
                
                # Include all detections regardless of class
                # Prepare detection data
                detection = {
                    "label": label,
                    "score": score,
                    "box": {
                        "xmin": float(box[0]),
                        "ymin": float(box[1]),
                        "xmax": float(box[2]),
                        "ymax": float(box[3])
                    }
                }
                
                # Add mask data if available
                if masks is not None and i < len(masks):
                    # Convert mask tensor to a 2D array of 0s and 1s
                    mask_tensor = masks[i]
                    # Convert to binary mask and then to list format
                    mask_array = (mask_tensor > 0.5).astype(int).tolist()
                    detection["mask"] = mask_array
                
                detections.append(detection)
        
        return jsonify({
            "success": True,
            "detections": detections
        })
    
    except Exception as e:
        app.logger.error(f"Error in detect_food_stream: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/upload_video', methods=['POST'])
def upload_video():
    """
    Handle video file uploads and save them with timestamps.
    
    Expected form data:
    - video: video file (webm format)
    - timestamp: timestamp string from client
    
    Returns:
    {
        "success": true,
        "filename": "saved_filename.webm",
        "timestamp": "timestamp_string",
        "path": "path/to/saved/file"
    }
    """
    try:
        # Log request received
        print(f"[VIDEO UPLOAD] Request received at {datetime.now().isoformat()}")
        print(f"[VIDEO UPLOAD] Request headers: {dict(request.headers)}")
        print(f"[VIDEO UPLOAD] Request form data keys: {list(request.form.keys())}")
        print(f"[VIDEO UPLOAD] Request files keys: {list(request.files.keys())}")
        
        # Check if video file is in the request
        if 'video' not in request.files:
            print(f"[VIDEO UPLOAD] ERROR: No video file in request")
            return jsonify({"success": False, "error": "No video file provided"}), 400
        
        video_file = request.files['video']
        print(f"[VIDEO UPLOAD] Video file received: {video_file.filename}")
        print(f"[VIDEO UPLOAD] Video file content type: {video_file.content_type}")
        
        if video_file.filename == '':
            print(f"[VIDEO UPLOAD] ERROR: Empty filename")
            return jsonify({"success": False, "error": "No video file selected"}), 400
        
        # Get timestamp from form data or generate one
        timestamp = request.form.get('timestamp', datetime.now().isoformat())
        print(f"[VIDEO UPLOAD] Client timestamp: {timestamp}")
        
        # Secure the filename and ensure it has the correct extension
        filename = werkzeug.utils.secure_filename(video_file.filename)
        # if not filename.lower().endswith('.webm'):
        #     filename += '.webm'
        
        # Add timestamp prefix to ensure unique ordering
        timestamp_prefix = timestamp.replace(':', '-').replace('.', '-')
        saved_filename = f"{timestamp_prefix}_{filename}"
        
        # Save the file
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], saved_filename)
        print(f"[VIDEO UPLOAD] Saving file to: {file_path}")
        
        video_file.save(file_path)
        
        # Get file size for logging
        file_size = os.path.getsize(file_path)
        print(f"[VIDEO UPLOAD] File saved successfully: {saved_filename}")
        print(f"[VIDEO UPLOAD] File size: {file_size} bytes ({file_size / 1024 / 1024:.2f} MB)")
        print(f"[VIDEO UPLOAD] Full path: {os.path.abspath(file_path)}")
        
        # List files in upload directory for verification
        try:
            existing_files = os.listdir(app.config['UPLOAD_FOLDER'])
            print(f"[VIDEO UPLOAD] Total files in upload directory: {len(existing_files)}")
            if len(existing_files) <= 5:  # Only list if not too many files
                print(f"[VIDEO UPLOAD] Files: {sorted(existing_files)}")
        except Exception as e:
            print(f"[VIDEO UPLOAD] Warning: Could not list directory contents: {e}")
        
        print(f"[VIDEO UPLOAD] Upload completed successfully at {datetime.now().isoformat()}")
        
        # Trigger Gemini video analysis in background
        gemini_thread = threading.Thread(target=run_gemini_video_analysis, args=(file_path,), daemon=True)
        gemini_thread.start()
        print(f"[GEMINI] Started video analysis for: {saved_filename}")
        
        return jsonify({
            "success": True,
            "filename": saved_filename,
            "timestamp": timestamp,
            "path": file_path,
            "size": file_size
        })
    
    except Exception as e:
        print(f"[VIDEO UPLOAD] ERROR: {str(e)}")
        print(f"[VIDEO UPLOAD] Error type: {type(e).__name__}")
        import traceback
        print(f"[VIDEO UPLOAD] Traceback: {traceback.format_exc()}")
        app.logger.error(f"Error in upload_video: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/upload_audio', methods=['POST'])
def upload_audio():
    """
    Handle audio file uploads and save them with timestamps.
    
    Expected form data:
    - audio: audio file (webm format)
    - timestamp: timestamp string from client
    
    Returns:
    {
        "success": true,
        "filename": "saved_filename.webm",
        "timestamp": "timestamp_string",
        "path": "path/to/saved/file"
    }
    """
    try:
        # Log request received
        print(f"[AUDIO UPLOAD] Request received at {datetime.now().isoformat()}")
        print(f"[AUDIO UPLOAD] Request headers: {dict(request.headers)}")
        print(f"[AUDIO UPLOAD] Request form data keys: {list(request.form.keys())}")
        print(f"[AUDIO UPLOAD] Request files keys: {list(request.files.keys())}")
        
        # Check if audio file is in the request
        if 'audio' not in request.files:
            print(f"[AUDIO UPLOAD] ERROR: No audio file in request")
            return jsonify({"success": False, "error": "No audio file provided"}), 400
        
        audio_file = request.files['audio']
        print(f"[AUDIO UPLOAD] Audio file received: {audio_file.filename}")
        print(f"[AUDIO UPLOAD] Audio file content type: {audio_file.content_type}")
        
        if audio_file.filename == '':
            print(f"[AUDIO UPLOAD] ERROR: Empty filename")
            return jsonify({"success": False, "error": "No audio file selected"}), 400
        
        # Get timestamp from form data or generate one
        timestamp = request.form.get('timestamp', datetime.now().isoformat())
        print(f"[AUDIO UPLOAD] Client timestamp: {timestamp}")
        
        # Secure the filename and ensure it has the correct extension
        filename = werkzeug.utils.secure_filename(audio_file.filename)
        if not filename.lower().endswith('.webm'):
            filename += '.webm'
        
        # Add timestamp prefix to ensure unique ordering
        timestamp_prefix = timestamp.replace(':', '-').replace('.', '-')
        saved_filename = f"{timestamp_prefix}_{filename}"
        
        # Save the file
        file_path = os.path.join(app.config['AUDIO_UPLOAD_FOLDER'], saved_filename)
        print(f"[AUDIO UPLOAD] Saving file to: {file_path}")
        
        audio_file.save(file_path)
        
        # Get file size for logging
        file_size = os.path.getsize(file_path)
        print(f"[AUDIO UPLOAD] File saved successfully: {saved_filename}")
        print(f"[AUDIO UPLOAD] File size: {file_size} bytes ({file_size / 1024:.2f} KB)")
        print(f"[AUDIO UPLOAD] Full path: {os.path.abspath(file_path)}")
        
        # List files in upload directory for verification
        try:
            existing_files = os.listdir(app.config['AUDIO_UPLOAD_FOLDER'])
            print(f"[AUDIO UPLOAD] Total files in audio upload directory: {len(existing_files)}")
            if len(existing_files) <= 5:  # Only list if not too many files
                print(f"[AUDIO UPLOAD] Files: {sorted(existing_files)}")
        except Exception as e:
            print(f"[AUDIO UPLOAD] Warning: Could not list directory contents: {e}")
        
        print(f"[AUDIO UPLOAD] Upload completed successfully at {datetime.now().isoformat()}")
        
        # Trigger Gemini audio analysis in background
        gemini_thread = threading.Thread(target=run_gemini_audio_analysis, args=(file_path,), daemon=True)
        gemini_thread.start()
        print(f"[GEMINI] Started audio analysis for: {saved_filename}")
        
        return jsonify({
            "success": True,
            "filename": saved_filename,
            "timestamp": timestamp,
            "path": file_path,
            "size": file_size
        })
    
    except Exception as e:
        print(f"[AUDIO UPLOAD] ERROR: {str(e)}")
        print(f"[AUDIO UPLOAD] Error type: {type(e).__name__}")
        import traceback
        print(f"[AUDIO UPLOAD] Traceback: {traceback.format_exc()}")
        app.logger.error(f"Error in upload_audio: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({'status': 'healthy', 'model_loaded': True})


@app.route('/classes', methods=['GET'])
def get_classes():
    """Return the list of available food classes"""
    try:
        class_names = list(model.names.values()) if hasattr(model, 'names') else []
        return jsonify({
            'success': True,
            'classes': class_names,
            'count': len(class_names)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# WebSocket events
@socketio.on('connect')
def handle_connect():
    print(f'[STREAM] Client connected: {request.sid}')
    connected_clients.add(request.sid)
    emit('status', {'message': 'Connected to stream'})

@socketio.on('disconnect')
def handle_disconnect():
    print(f'[STREAM] Client disconnected: {request.sid}')
    connected_clients.discard(request.sid)

@socketio.on('heartbeat')
def handle_heartbeat():
    emit('heartbeat_response', {'timestamp': datetime.now().isoformat()})

def broadcast_to_clients(message_type, content):
    """Broadcast message to all connected clients"""
    if connected_clients:
        message = {
            'type': message_type,
            'content': content,
            'timestamp': datetime.now().isoformat()
        }
        socketio.emit('stream_update', message)
        print(f'[STREAM] Broadcasted {message_type} to {len(connected_clients)} clients')

def generate_sensor_data():
    """Generate fake sensor data like from a sport watch"""
    return {
        'energy': random.randint(20, 100),
        'sleep': f"{random.uniform(6, 9):.2f} Hours",
        'sport': f"{random.randint(1000, 15000)} Steps",
        'blood': f"{random.randint(70, 120)}",
        'heartRate': f"{random.randint(60, 100)}",
        'calories': f"{random.randint(200, 800)}"
    }

def generate_sensor_html():
    """Generate plain text for sensor values"""
    data = generate_sensor_data()
    return f"""èƒ½é‡å€¼: {data['energy']}
ç¡çœ æ—¶é•¿: {data['sleep']} å°æ—¶
è¿åŠ¨æ­¥æ•°: {data['sport']} æ­¥
è¡€åŽ‹: {data['blood']}
å¿ƒçŽ‡: {data['heartRate']} æ¬¡/åˆ†
å¡è·¯é‡Œ: {data['calories']} å¡"""

def generate_emotion_html():
    """Generate plain text for user emotions from stored analysis results"""
    if analysis_results['emotion_history']:
        # Get the most recent emotion analysis
        latest_emotion = analysis_results['emotion_history'][-1]
        return f"""æƒ…ç»ªçŠ¶æ€: {latest_emotion['emotion']}
å¼ºåº¦: {latest_emotion['intensity']}
æ£€æµ‹æ—¶é—´: {datetime.fromisoformat(latest_emotion['timestamp']).strftime('%H:%M:%S')}"""
    else:
        print("[EMOTION] No emotion analysis results available", analysis_results['emotion_history'])
        # Fallback if no analysis results available
        return "æƒ…ç»ªçŠ¶æ€: ç­‰å¾…åˆ†æžæ•°æ®..."

def generate_audio_detection_html():
    """Generate plain text for audio detection results from stored analysis results"""
    if analysis_results['audio_analysis']:
        # Get the most recent audio analysis
        latest_audio = analysis_results['audio_analysis'][-1]
        return f"""è¯­éŸ³å†…å®¹: {latest_audio['speech_content']}
æƒ…ç»ªçŠ¶æ€: {latest_audio['emotion_state']}
éŸ³é‡: {latest_audio['volume_level']}
éŸ³è´¨: {latest_audio['voice_quality']}
åˆ†æžæ—¶é—´: {datetime.fromisoformat(latest_audio['timestamp']).strftime('%H:%M:%S')}"""
    else:
        print("[AUDIO] No audio analysis results available", analysis_results['audio_analysis'])
        # Fallback if no analysis results available
        return "éŸ³é¢‘åˆ†æž: ç­‰å¾…åˆ†æžæ•°æ®..."

def generate_video_detection_html():
    """Generate plain text for video detection results from stored analysis results"""
    if analysis_results['video_analysis']:
        # Get the most recent video analysis
        latest_video = analysis_results['video_analysis'][-1]
        return f"""æ´»åŠ¨å¼ºåº¦: {latest_video['activity_level']}
å¿ƒç†çŠ¶æ€: {latest_video['overall_state']}
æƒ…ç»ªå¼ºåº¦: {latest_video['emotion_intensity']}
ä¸»è¦æƒ…ç»ª: {latest_video['emotion_state']}
åˆ†æžæ—¶é—´: {datetime.fromisoformat(latest_video['timestamp']).strftime('%H:%M:%S')}"""
    else:
        # Fallback if no analysis results available
        return "è§†é¢‘åˆ†æž: ç­‰å¾…åˆ†æžæ•°æ®..."

def generate_overall_status_html():
    """Generate plain text for overall user status from stored analysis results"""
    total_analyses = len(analysis_results['video_analysis']) + len(analysis_results['audio_analysis'])
    
    if total_analyses > 0:
        # Get recent analyses for summary
        recent_emotions = [e['emotion'] for e in analysis_results['emotion_history'][-5:]]
        recent_activities = [a['activity'] for a in analysis_results['activity_history'][-5:]]
        
        # Determine most common emotion and activity
        if recent_emotions:
            most_common_emotion = max(set(recent_emotions), key=recent_emotions.count)
        else:
            most_common_emotion = "æœªçŸ¥"
            
        if recent_activities:
            most_common_activity = max(set(recent_activities), key=recent_activities.count)
        else:
            most_common_activity = "æœªçŸ¥"
        
        return f"""æ•´ä½“çŠ¶æ€: åŸºäºŽæœ€è¿‘{total_analyses}æ¬¡åˆ†æž
ä¸»è¦æƒ…ç»ª: {most_common_emotion}
ä¸»è¦æ´»åŠ¨: {most_common_activity}
æ•°æ®æ›´æ–°: {datetime.now().strftime('%H:%M:%S')}
åˆ†æžæ¬¡æ•°: è§†é¢‘{len(analysis_results['video_analysis'])}æ¬¡, éŸ³é¢‘{len(analysis_results['audio_analysis'])}æ¬¡"""
    else:
        # Fallback if no analysis results available
        return "æ•´ä½“çŠ¶æ€: ç­‰å¾…åˆ†æžæ•°æ®..."

def sensor_update_timer():
    """Timer for updating sensor values every 1 second"""
    while True:
        try:
            html_content = generate_sensor_html()
            broadcast_to_clients('sensor_update', html_content)
            time.sleep(1)
        except Exception as e:
            print(f'[STREAM] Error in sensor timer: {e}')
            time.sleep(1)

def emotion_update_timer():
    """Timer for updating emotions every 3 seconds"""
    while True:
        try:
            html_content = generate_emotion_html()
            broadcast_to_clients('emotion_update', html_content)
            time.sleep(3)
        except Exception as e:
            print(f'[STREAM] Error in emotion timer: {e}')
            time.sleep(3)

def overall_status_timer():
    """Timer for updating overall status every 20 seconds"""
    while True:
        try:
            # Generate comprehensive summary using stored analysis results
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                summary_text = loop.run_until_complete(generate_comprehensive_summary())
                summary_html = generate_summary_html(summary_text)
                broadcast_to_clients('overall_status', summary_html)
            finally:
                loop.close()
            time.sleep(20)
        except Exception as e:
            print(f'[STREAM] Error in overall status timer: {e}')
            time.sleep(20)

def comprehensive_summary_timer():
    """Timer for generating comprehensive summaries every 5 minutes"""
    while True:
        try:
            print('[SUMMARY] Starting comprehensive summary generation...')
            # Generate comprehensive summary using stored analysis results
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                summary_text = loop.run_until_complete(generate_comprehensive_summary())
                summary_html = generate_summary_html(summary_text)
                broadcast_to_clients('overall_status', summary_html)
                print('[SUMMARY] Comprehensive summary generated and broadcasted')
            finally:
                loop.close()
            time.sleep(300)  # 5 minutes
        except Exception as e:
            print(f'[SUMMARY] Error in comprehensive summary timer: {e}')
            time.sleep(300)

async def analyze_video_with_gemini(video_path):
    """Analyze video using Gemini model"""
    try:
        if not gemini_client:
            print('[GEMINI] Client not available, using fallback')
            # Fallback to mock data
            html_content = generate_video_detection_html()
            broadcast_to_clients('video_detect', html_content)
            emotion_html = generate_emotion_html()
            broadcast_to_clients('emotion_update', emotion_html)
            return
        
        print(f'[GEMINI] Analyzing video: {video_path}')
        
        # Check if file exists and get file info
        import os
        if not os.path.exists(video_path):
            print(f'[GEMINI] Error: Video file does not exist: {video_path}')
            return
        
        file_size = os.path.getsize(video_path)
        print(f'[GEMINI] Video file info - Path: {video_path}, Size: {file_size} bytes')
        
        # Test Gemini client connectivity
        try:
            print(f'[GEMINI] Testing client connectivity...')
            test_response = gemini_client.chat.completions.create(
                model='gemini-3-pro-preview',
                messages=[{"role": "user", "content": "Hello, this is a test."}],
                max_tokens=50
            )
            print(f'[GEMINI] Client connectivity test passed')
        except Exception as e:
            print(f'[GEMINI] Client connectivity test failed: {e}')
            return
        
        # Read and encode video file
        try:
            with open(video_path, "rb") as f:
                video_b64 = base64.b64encode(f.read()).decode()
                video_url = f"data:video/mp4;base64,{video_b64}"
            print(f'[GEMINI] Video encoded successfully')
        except Exception as e:
            print(f'[GEMINI] Error encoding video: {e}')
            return
        
        # Analyze video content
        analysis_prompt = """è¯·è¯¦ç»†åˆ†æžè¿™ä¸ªè§†é¢‘ä¸­çš„äººç‰©ï¼Œæä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **é¢éƒ¨è¡¨æƒ…åˆ†æž**ï¼š
   - ä¸»è¦æƒ…ç»ªçŠ¶æ€ï¼ˆå¿«ä¹ã€æ‚²ä¼¤ã€æ„¤æ€’ã€æƒŠè®¶ã€ææƒ§ã€åŽŒæ¶ã€ä¸­æ€§ç­‰ï¼‰
   - æƒ…ç»ªå¼ºåº¦ï¼ˆ1-10åˆ†ï¼‰
   - è¡¨æƒ…å˜åŒ–æè¿°

2. **è‚¢ä½“è¯­è¨€åˆ†æž**ï¼š
   - å§¿æ€å’ŒåŠ¨ä½œæè¿°
   - èº«ä½“è¯­è¨€ä¼ è¾¾çš„æƒ…ç»ª
   - æ´»åŠ¨å¼ºåº¦ï¼ˆä½Ž/ä¸­/é«˜ï¼‰

3. **çŽ¯å¢ƒä¸Žæ´»åŠ¨**ï¼š
   - å½“å‰ä¸»è¦æ´»åŠ¨
   - çŽ¯å¢ƒæè¿°
   - é¥®é£ŸçŠ¶å†µä¸ŽCalorieä¸Žå¥åº·

4. **ç»¼åˆæƒ…ç»ªè¯„ä¼°**ï¼š
   - æ•´ä½“å¿ƒç†çŠ¶æ€
   - æ³¨æ„åŠ›é›†ä¸­ç¨‹åº¦
   - å¯èƒ½çš„æƒ…ç»ªåŽŸå› 

è¯·ç”¨ç»“æž„åŒ–çš„ä¸­æ–‡å›žç­”ï¼Œæ¯ä¸ªéƒ¨åˆ†è¯¦ç»†æè¿°ã€‚"""
        
        response = gemini_client.chat.completions.create(
            model="gemini-3-pro-preview",
            messages=[
                {"role": "system", "content": "You are a helpful assistant specialized in video analysis."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": analysis_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": video_url
                            },
                            "mime_type": "video/mp4",
                        }
                    ]
                }
            ],
            temperature=0.2,
            max_tokens=4096
        )
        
        analysis_text = response.choices[0].message.content
        print(f'[GEMINI] Video analysis completed successfully')
        
        # Parse detailed analysis and generate HTML
        # Extract key information from the structured response
        emotion_state = "æœªæ£€æµ‹åˆ°"
        emotion_intensity = "æœªçŸ¥"
        activity_level = "æœªçŸ¥"
        overall_state = "æ­£å¸¸"
        
        # Parse the analysis text for key information
        if "é¢éƒ¨è¡¨æƒ…åˆ†æž" in analysis_text:
            lines = analysis_text.split('\n')
            for i, line in enumerate(lines):
                if "ä¸»è¦æƒ…ç»ªçŠ¶æ€" in line:
                    emotion_state = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else emotion_state
                elif "æƒ…ç»ªå¼ºåº¦" in line:
                    emotion_intensity = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else emotion_intensity
                elif "æ´»åŠ¨å¼ºåº¦" in line:
                    activity_level = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else activity_level
                elif "æ•´ä½“å¿ƒç†çŠ¶æ€" in line:
                    overall_state = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else overall_state
        
        # Generate comprehensive video detection text
        video_text = f"æ´»åŠ¨å¼ºåº¦: {activity_level}\nå¿ƒç†çŠ¶æ€: {overall_state}\næƒ…ç»ªå¼ºåº¦: {emotion_intensity}\nåˆ†æžæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"
        broadcast_to_clients('video_detect', video_text)
        
        # Generate detailed emotion text
        emotion_text = f"ä¸»è¦æƒ…ç»ª: {emotion_state}\næ£€æµ‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\nåˆ†æžæ¨¡åž‹: Gemini-3-Pro"
        broadcast_to_clients('emotion_update', emotion_text)
        
        # Store analysis results for summary generation
        video_result = {
            'timestamp': datetime.now().isoformat(),
            'emotion_state': emotion_state,
            'emotion_intensity': emotion_intensity,
            'activity_level': activity_level,
            'overall_state': overall_state,
            'full_analysis': analysis_text,
            'file_path': video_path
        }
        
        # Add to storage with size limit
        analysis_results['video_analysis'].append(video_result)
        print(f'[STORAGE] Stored video analysis result: {analysis_results["video_analysis"][-1]}')
        if len(analysis_results['video_analysis']) > MAX_STORED_RESULTS:
            analysis_results['video_analysis'].pop(0)
        
        # Store emotion history
        emotion_entry = {
            'timestamp': datetime.now().isoformat(),
            'source': 'video',
            'emotion': emotion_state,
            'intensity': emotion_intensity,
            'confidence': 'high'
        }
        analysis_results['emotion_history'].append(emotion_entry)
        if len(analysis_results['emotion_history']) > MAX_STORED_RESULTS:
            analysis_results['emotion_history'].pop(0)
        
        # Store activity history
        activity_entry = {
            'timestamp': datetime.now().isoformat(),
            'source': 'video',
            'activity': overall_state,
            'intensity': activity_level
        }
        analysis_results['activity_history'].append(activity_entry)
        if len(analysis_results['activity_history']) > MAX_STORED_RESULTS:
            analysis_results['activity_history'].pop(0)
        
        print(f'[STORAGE] Stored video analysis results. Total stored: {len(analysis_results["video_analysis"])}')
        
        # Clean up uploaded file
        try:
            gemini_client.files.delete(uploaded_file.name)
            print(f'[GEMINI] Cleaned up video file: {uploaded_file.name}')
        except:
            pass
        
        print(f'[GEMINI] Video analysis completed for: {video_path}')
        
    except Exception as e:
        print(f'[GEMINI] Error analyzing video: {e}')
        # Fallback to mock data
        html_content = generate_video_detection_html()
        broadcast_to_clients('video_detect', html_content)
        emotion_html = generate_emotion_html()
        broadcast_to_clients('emotion_update', emotion_html)

async def generate_comprehensive_summary():
    """Generate comprehensive summary using stored analysis results"""
    try:
        if not gemini_client:
            print('[SUMMARY] Gemini client not available, using fallback')
            return generate_mock_summary()
        
        # Check if we have enough data for meaningful summary
        total_analyses = len(analysis_results['video_analysis']) + len(analysis_results['audio_analysis'])
        if total_analyses < 2:
            print('[SUMMARY] Insufficient data for comprehensive summary')
            return generate_mock_summary()
        
        print(f'[SUMMARY] Generating comprehensive summary from {total_analyses} analyses')
        
        # Prepare data for summary
        recent_video = analysis_results['video_analysis'][-5:]  # Last 5 video analyses
        recent_audio = analysis_results['audio_analysis'][-5:]  # Last 5 audio analyses
        emotion_history = analysis_results['emotion_history'][-10:]  # Last 10 emotion entries
        activity_history = analysis_results['activity_history'][-10:]  # Last 10 activity entries
        
        # Build summary prompt
        summary_prompt = f"""åŸºäºŽä»¥ä¸‹åˆ†æžç»“æžœï¼Œè¯·ç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§çš„ç”¨æˆ·çŠ¶æ€æ€»ç»“ï¼š

## è§†é¢‘åˆ†æžç»“æžœï¼ˆæœ€è¿‘{len(recent_video)}æ¬¡ï¼‰ï¼š
{chr(10).join([f"- {v['timestamp']}: æƒ…ç»ª={v['emotion_state']}, å¼ºåº¦={v['emotion_intensity']}, çŠ¶æ€={v['overall_state']}" for v in recent_video])}

## éŸ³é¢‘åˆ†æžç»“æžœï¼ˆæœ€è¿‘{len(recent_audio)}æ¬¡ï¼‰ï¼š
{chr(10).join([f"- {a['timestamp']}: æƒ…ç»ª={a['emotion_state']}, éŸ³é‡={a['volume_level']}, å†…å®¹={a['speech_content']}" for a in recent_audio])}

## æƒ…ç»ªåŽ†å²ï¼ˆæœ€è¿‘{len(emotion_history)}æ¬¡ï¼‰ï¼š
{chr(10).join([f"- {e['timestamp']}: {e['source']} - {e['emotion']} (å¼ºåº¦: {e['intensity']})" for e in emotion_history])}

## æ´»åŠ¨åŽ†å²ï¼ˆæœ€è¿‘{len(activity_history)}æ¬¡ï¼‰ï¼š
{chr(10).join([f"- {a['timestamp']}: {a['source']} - {a['activity']}" for a in activity_history])}

è¯·åŸºäºŽä»¥ä¸Šæ•°æ®ï¼Œæä¾›ä»¥ä¸‹æ€»ç»“ï¼š

1. **æ•´ä½“æƒ…ç»ªè¶‹åŠ¿åˆ†æž**ï¼š
   - ä¸»è¦æƒ…ç»ªæ¨¡å¼
   - æƒ…ç»ªå˜åŒ–è¶‹åŠ¿
   - æƒ…ç»ªç¨³å®šæ€§è¯„ä¼°

2. **æ´»åŠ¨æ¨¡å¼åˆ†æž**ï¼š
   - ä¸»è¦æ´»åŠ¨ç±»åž‹
   - æ´»åŠ¨å¼ºåº¦å˜åŒ–
   - å·¥ä½œ/ä¼‘æ¯å¹³è¡¡

3. **æ²Ÿé€šæ¨¡å¼åˆ†æž**ï¼š
   - è¯­éŸ³æ´»åŠ¨é¢‘çŽ‡
   - æ²Ÿé€šæƒ…ç»ªç‰¹å¾
   - è¡¨è¾¾æ–¹å¼ç‰¹ç‚¹

4. **ç»¼åˆçŠ¶æ€è¯„ä¼°**ï¼š
   - å½“å‰æ•´ä½“çŠ¶æ€
   - åŽ‹åŠ›æ°´å¹³è¯„ä¼°
   - æ•ˆçŽ‡çŠ¶æ€è¯„ä¼°

5. **å»ºè®®å’Œæé†’**ï¼š
   - å¥åº·å»ºè®®
   - å·¥ä½œæ•ˆçŽ‡å»ºè®®
   - æƒ…ç»ªç®¡ç†å»ºè®®

è¯·ç”¨ç»“æž„åŒ–çš„ä¸­æ–‡å›žç­”ï¼Œæ¯ä¸ªéƒ¨åˆ†è¯¦ç»†åˆ†æžå¹¶æä¾›å…·ä½“å»ºè®®ã€‚æ¯ä¸ªéƒ¨åˆ†åªè¾“å‡ºä¸€å¥ç®€çŸ­æè¿°ã€‚"""

        # Generate summary using Gemini
        response = gemini_client.chat.completions.create(
            model="gemini-3-pro-preview",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.3,
            max_tokens=4096
        )
        
        summary_text = response.choices[0].message.content
        print(f'[SUMMARY] Comprehensive summary generated successfully')
        
        return summary_text
        
    except Exception as e:
        print(f'[SUMMARY] Error generating comprehensive summary: {e}')
        return generate_mock_summary()

def generate_mock_summary():
    """Generate mock summary when Gemini is unavailable"""
    import random
    
    emotion_states = ['ä¸“æ³¨', 'å¹³é™', 'ç§¯æž', 'æ”¾æ¾', 'æŠ•å…¥']
    activity_levels = ['ä¸­ç­‰', 'è¾ƒé«˜', 'é€‚ä¸­', 'ç¨³å®š']
    
    mock_summary = f"""
## ç»¼åˆçŠ¶æ€æ€»ç»“

### 1. æ•´ä½“æƒ…ç»ªè¶‹åŠ¿åˆ†æž
- **ä¸»è¦æƒ…ç»ªæ¨¡å¼**: {random.choice(emotion_states)}
- **æƒ…ç»ªå˜åŒ–è¶‹åŠ¿**: æƒ…ç»ªçŠ¶æ€ç›¸å¯¹ç¨³å®š
- **æƒ…ç»ªç¨³å®šæ€§è¯„ä¼°**: è‰¯å¥½

### 2. æ´»åŠ¨æ¨¡å¼åˆ†æž
- **ä¸»è¦æ´»åŠ¨ç±»åž‹**: ç”µè„‘å·¥ä½œ
- **æ´»åŠ¨å¼ºåº¦å˜åŒ–**: {random.choice(activity_levels)}
- **å·¥ä½œ/ä¼‘æ¯å¹³è¡¡**: éœ€è¦æ³¨æ„é€‚å½“ä¼‘æ¯

### 3. æ²Ÿé€šæ¨¡å¼åˆ†æž
- **è¯­éŸ³æ´»åŠ¨é¢‘çŽ‡**: é€‚ä¸­
- **æ²Ÿé€šæƒ…ç»ªç‰¹å¾**: ç§¯æžæ­£é¢
- **è¡¨è¾¾æ–¹å¼ç‰¹ç‚¹**: æ¸…æ™°æµç•…

### 4. ç»¼åˆçŠ¶æ€è¯„ä¼°
- **å½“å‰æ•´ä½“çŠ¶æ€**: è‰¯å¥½
- **åŽ‹åŠ›æ°´å¹³è¯„ä¼°**: ä¸­ç­‰
- **æ•ˆçŽ‡çŠ¶æ€è¯„ä¼°**: é«˜æ•ˆ

### 5. å»ºè®®å’Œæé†’
- **å¥åº·å»ºè®®**: å®šæ—¶ä¼‘æ¯ï¼Œä¿æŠ¤è§†åŠ›
- **å·¥ä½œæ•ˆçŽ‡å»ºè®®**: ä¿æŒä¸“æ³¨ï¼Œé€‚å½“è°ƒæ•´
- **æƒ…ç»ªç®¡ç†å»ºè®®**: ä¿æŒç§¯æžå¿ƒæ€

*æ³¨ï¼šæ­¤ä¸ºæ¨¡æ‹Ÿæ€»ç»“ï¼Œå®žé™…åˆ†æžéœ€è¦æ›´å¤šæ•°æ®*
"""
    
    return mock_summary

def generate_summary_html(summary_text):
    """Generate plain text for summary display"""
    # Convert markdown-style headers to plain text
    import re
    
    # Convert headers to plain text with separators
    summary_text = re.sub(r'^### (.*?)$', r'---\n\1\n---', summary_text, flags=re.MULTILINE)
    summary_text = re.sub(r'^## (.*?)$', r'===\n\1\n===', summary_text, flags=re.MULTILINE)
    summary_text = re.sub(r'^# (.*?)$', r'***\n\1\n***', summary_text, flags=re.MULTILINE)
    
    # Convert bold text to plain text
    summary_text = re.sub(r'\*\*(.*?)\*\*', r'\1', summary_text)
    
    # Convert list items to plain text with bullets
    summary_text = re.sub(r'^- (.*?)$', r'â€¢ \1', summary_text, flags=re.MULTILINE)
    
    # Clean up extra line breaks
    summary_text = re.sub(r'\n{3,}', '\n\n', summary_text)
    
    # Wrap in container with simple format
    text_content = f"""ðŸ“Š ç»¼åˆçŠ¶æ€æ€»ç»“
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}
{summary_text}"""
    
    return text_content

async def analyze_audio_with_gemini(audio_path):
    """Analyze audio using Gemini model"""
    try:
        if not gemini_client:
            print('[GEMINI] Client not available, using fallback')
            # Fallback to mock data
            html_content = generate_audio_detection_html()
            broadcast_to_clients('audio_detect', html_content)
            return
        
        print(f'[GEMINI] Analyzing audio: {audio_path}')
        
        # Check if file exists and get file info
        import os
        if not os.path.exists(audio_path):
            print(f'[GEMINI] Error: Audio file does not exist: {audio_path}')
            return
        
        file_size = os.path.getsize(audio_path)
        print(f'[GEMINI] Audio file info - Path: {audio_path}, Size: {file_size} bytes')
        
        # Read and encode audio file
        try:
            with open(audio_path, "rb") as f:
                audio_b64 = base64.b64encode(f.read()).decode()
                audio_url = f"data:audio/webm;base64,{audio_b64}"
            print(f'[GEMINI] Audio encoded successfully')
        except Exception as e:
            print(f'[GEMINI] Error encoding audio: {e}')
            return
        
        # Transcribe and analyze audio content
        analysis_prompt = """è¯·è¯¦ç»†åˆ†æžè¿™æ®µéŸ³é¢‘ä¸­çš„è¯­éŸ³å’Œå£°éŸ³ï¼Œæä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **è¯­éŸ³å†…å®¹åˆ†æž**ï¼š
   - å®Œæ•´è½¬å½•æ–‡æœ¬
   - è¯­è¨€å’Œå£éŸ³ç‰¹å¾
   - è¯­é€Ÿå’Œæµç•…åº¦

2. **å£°éŸ³æƒ…ç»ªåˆ†æž**ï¼š
   - ä¸»è¦æƒ…ç»ªçŠ¶æ€ï¼ˆå¿«ä¹ã€æ‚²ä¼¤ã€æ„¤æ€’ã€ç„¦è™‘ã€å…´å¥‹ã€å¹³é™ç­‰ï¼‰
   - æƒ…ç»ªå¼ºåº¦ï¼ˆ1-10åˆ†ï¼‰
   - è¯­è°ƒå˜åŒ–å’Œæƒ…æ„Ÿè‰²å½©

3. **å£°éŸ³ç‰¹å¾**ï¼š
   - éŸ³é‡æ°´å¹³ï¼ˆä½Ž/ä¸­/é«˜ï¼‰
   - éŸ³è°ƒé«˜ä½Ž
   - å£°éŸ³æ¸…æ™°åº¦

4. **çŽ¯å¢ƒå£°éŸ³**ï¼š
   - èƒŒæ™¯å™ªéŸ³æè¿°
   - çŽ¯å¢ƒéŸ³æ•ˆ
   - éŸ³é¢‘è´¨é‡è¯„ä¼°

5. **ç»¼åˆå¿ƒç†çŠ¶æ€**ï¼š
   - è¯´è¯è€…å¿ƒç†çŠ¶æ€æŽ¨æ–­
   - æƒ…ç»ªç¨³å®šæ€§
   - å¯èƒ½çš„æƒ…ç»ªè§¦å‘å› ç´ 

è¯·ç”¨ç»“æž„åŒ–çš„ä¸­æ–‡å›žç­”ï¼Œæ¯ä¸ªéƒ¨åˆ†è¯¦ç»†åˆ†æžã€‚"""
        
        response = gemini_client.chat.completions.create(
            model="gemini-3-pro-preview",
            messages=[
                {"role": "system", "content": "You are a helpful assistant specialized in audio analysis."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": analysis_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": audio_url
                            },
                            "mime_type": "audio/webm",
                        }
                    ]
                }
            ],
            temperature=0.2,
            max_tokens=4096
        )
        
        analysis_text = response.choices[0].message.content
        print(f'[GEMINI] Audio analysis completed successfully')
        
        # Parse detailed analysis and generate HTML
        # Extract key information from the structured response
        speech_content = "æ— è¯­éŸ³"
        emotion_state = "æœªæ£€æµ‹åˆ°"
        emotion_intensity = "æœªçŸ¥"
        volume_level = "æœªçŸ¥"
        voice_quality = "æ­£å¸¸"
        
        # Parse the analysis text for key information
        if "è¯­éŸ³å†…å®¹åˆ†æž" in analysis_text or "å£°éŸ³æƒ…ç»ªåˆ†æž" in analysis_text:
            lines = analysis_text.split('\n')
            for i, line in enumerate(lines):
                if "è½¬å½•æ–‡æœ¬" in line or "å®Œæ•´è½¬å½•æ–‡æœ¬" in line:
                    speech_content = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else speech_content
                    # Limit transcript length for display
                    if len(speech_content) > 50:
                        speech_content = speech_content[:50] + "..."
                elif "ä¸»è¦æƒ…ç»ªçŠ¶æ€" in line:
                    emotion_state = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else emotion_state
                elif "æƒ…ç»ªå¼ºåº¦" in line:
                    emotion_intensity = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else emotion_intensity
                elif "éŸ³é‡æ°´å¹³" in line:
                    volume_level = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else volume_level
                elif "å£°éŸ³æ¸…æ™°åº¦" in line:
                    voice_quality = line.split('ï¼š')[-1].strip() if 'ï¼š' in line else voice_quality
        
        # Generate comprehensive audio detection text
        audio_text = f"è¯­éŸ³å†…å®¹: {speech_content}\næƒ…ç»ªçŠ¶æ€: {emotion_state}\néŸ³é‡: {volume_level}\néŸ³è´¨: {voice_quality}\nåˆ†æžæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"
        broadcast_to_clients('audio_detect', audio_text)
        
        # Store analysis results for summary generation
        audio_result = {
            'timestamp': datetime.now().isoformat(),
            'speech_content': speech_content,
            'emotion_state': emotion_state,
            'emotion_intensity': emotion_intensity,
            'volume_level': volume_level,
            'voice_quality': voice_quality,
            'full_analysis': analysis_text,
            'file_path': audio_path
        }
        
        # Add to storage with size limit
        analysis_results['audio_analysis'].append(audio_result)
        if len(analysis_results['audio_analysis']) > MAX_STORED_RESULTS:
            analysis_results['audio_analysis'].pop(0)
        
        # Store emotion history from audio
        emotion_entry = {
            'timestamp': datetime.now().isoformat(),
            'source': 'audio',
            'emotion': emotion_state,
            'intensity': emotion_intensity,
            'confidence': 'high'
        }
        analysis_results['emotion_history'].append(emotion_entry)
        if len(analysis_results['emotion_history']) > MAX_STORED_RESULTS:
            analysis_results['emotion_history'].pop(0)
        
        # Store activity history from audio
        activity_entry = {
            'timestamp': datetime.now().isoformat(),
            'source': 'audio',
            'activity': f"è¯­éŸ³æ´»åŠ¨: {speech_content[:20]}..." if len(speech_content) > 20 else f"è¯­éŸ³æ´»åŠ¨: {speech_content}",
            'intensity': volume_level
        }
        analysis_results['activity_history'].append(activity_entry)
        if len(analysis_results['activity_history']) > MAX_STORED_RESULTS:
            analysis_results['activity_history'].pop(0)
        
        print(f'[STORAGE] Stored audio analysis results. Total stored: {len(analysis_results["audio_analysis"])}')
        
        # Clean up uploaded file
        try:
            gemini_client.files.delete(uploaded_file.name)
            print(f'[GEMINI] Cleaned up audio file: {uploaded_file.name}')
        except:
            pass
        
        print(f'[GEMINI] Audio analysis completed for: {audio_path}')
        
    except Exception as e:
        print(f'[GEMINI] Error analyzing audio: {e}')
        # Fallback to mock data
        html_content = generate_audio_detection_html()
        broadcast_to_clients('audio_detect', html_content)

def run_gemini_video_analysis(video_path):
    """Run video analysis in async context"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(analyze_video_with_gemini(video_path))
    finally:
        loop.close()

def run_gemini_audio_analysis(audio_path):
    """Run audio analysis in async context"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(analyze_audio_with_gemini(audio_path))
    finally:
        loop.close()


# Start background timers
def start_background_timers():
    """Start all background timer threads"""
    # Sensor values timer (1 second)
    sensor_thread = threading.Thread(target=sensor_update_timer, daemon=True)
    sensor_thread.start()
    print('[STREAM] Started sensor update timer (1s)')

    # Emotion update timer (3 seconds)
    emotion_thread = threading.Thread(target=emotion_update_timer, daemon=True)
    emotion_thread.start()
    print('[STREAM] Started emotion update timer (3s)')

    # Overall status timer (20 seconds)
    status_thread = threading.Thread(target=overall_status_timer, daemon=True)
    status_thread.start()
    print('[STREAM] Started overall status timer (20s)')

    # Comprehensive summary timer (5 minutes)
    summary_thread = threading.Thread(target=comprehensive_summary_timer, daemon=True)
    summary_thread.start()
    print('[SUMMARY] Started comprehensive summary timer (5m)')


if __name__ == '__main__':
    print("Starting YOLO food detection server with streaming...")
    
    # Start background timers
    start_background_timers()
    
    # Run the app with SocketIO
    socketio.run(app, host='0.0.0.0', port=5000, debug=False, allow_unsafe_werkzeug=True)
